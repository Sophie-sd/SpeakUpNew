#!/usr/bin/env python
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É news —Å—Ç–∞—Ç–µ–π –∑—ñ —Å—Ç–∞—Ä–æ–≥–æ —Å–∞–π—Ç—É speak-up.com.ua.
–í–∏–∫–æ–Ω—É—î—Ç—å—Å—è: python manage.py shell < scripts/import_news.py
–ê–±–æ: python manage.py runscript import_news (—è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è django-extensions)
"""
import os
import sys
import django
from datetime import datetime
from urllib.parse import urlparse, urljoin
import requests
from bs4 import BeautifulSoup

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Django
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, BASE_DIR)
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'SpeakUp.settings.develop')
django.setup()

from apps.core.models import NewsArticle
from apps.core.utils import (
    clean_wordpress_html,
    extract_images_from_html,
    download_and_save_image,
    update_html_image_urls
)
from apps.core.seo_config import PROGRAMS, LOCATIONS, CITIES

# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
OLD_SITE_BASE_URL = 'https://speak-up.com.ua'
NEW_SITE_BASE_URL = 'https://speak-up.com.ua'  # –ü—ñ—Å–ª—è –º—ñ–≥—Ä–∞—Ü—ñ—ó –¥–æ–º–µ–Ω—É
IMAGE_UPLOAD_PATH = 'news/images'


def get_all_news_urls():
    """
    –û—Ç—Ä–∏–º—É—î —Å–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö URL —Å—Ç–∞—Ç–µ–π –∑—ñ —Å—Ç–∞—Ä–æ–≥–æ —Å–∞–π—Ç—É.
    –°–ø–æ—á–∞—Ç–∫—É –Ω–∞–º–∞–≥–∞—î—Ç—å—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –∑ —Ñ–∞–π–ª—É, —è–∫—â–æ –Ω–µ–º–∞—î - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≤–±—É–¥–æ–≤–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫.
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    urls_file = os.path.join(script_dir, 'all_news_urls.txt')

    if os.path.exists(urls_file):
        print(f"üìñ –ß–∏—Ç–∞—é URL –∑ —Ñ–∞–π–ª—É: {urls_file}")
        with open(urls_file, 'r') as f:
            urls = [line.strip() for line in f if line.strip()]
        print(f"   –ó–Ω–∞–π–¥–µ–Ω–æ {len(urls)} URL –≤ —Ñ–∞–π–ª—ñ")
        return urls
    else:
        print("‚ö†Ô∏è  –§–∞–π–ª all_news_urls.txt –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é –≤–±—É–¥–æ–≤–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫.")
        return [
            '/news/anglijska-v-it-porady-yak-prokachaty-anglijsku-programistu/',
            '/news/czifri-ta-chisla-na-anglijskij-movi-navchitisya-rahuvati-legko-z-speak-up/',
            '/news/degrees-of-comparison-of-adjectives/',
            '/news/fraz-na-anglyjskom-dlya-obshhenyya-v-otele/',
            '/news/kolory-v-anglijskij-movi-osnovni-nazvy-prykmetnyky-idiomy-ta-vidtinky/',
            '/news/kuhonne-pryladdya-ta-stolovi-prybory-anglijskoyu/',
            '/news/mnozhyna-imennykiv-v-anglijskij-movi-yak-utvoryuyetsya-ta-yaki-ye-vynyatky/',
            '/news/nepravylni-diyeslova-v-anglijskij-movi-irregular-verbs/',
            '/news/pisni-na-zanyattyah-anglijskoyi-movy/',
            '/news/vse-pro-past-simple-yak-utvoryuyetsya-pravyla-vzhyvannya-pryklady/',
            '/news/yak-vyvchyty-anglijskyj-alfavit/',
            '/news/zapalyuyemo-bazhannya-vyvchaty-anglijsku/',
        ]


def parse_article(url_path, lang='uk'):
    """
    –ü–∞—Ä—Å–∏—Ç—å —Å—Ç–∞—Ç—Ç—é –∑—ñ —Å—Ç–∞—Ä–æ–≥–æ —Å–∞–π—Ç—É.

    Args:
        url_path: –®–ª—è—Ö –¥–æ —Å—Ç–∞—Ç—Ç—ñ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ /news/slug/)
        lang: –ú–æ–≤–∞ ('uk' –∞–±–æ 'ru')

    Returns:
        –°–ª–æ–≤–Ω–∏–∫ –∑ –¥–∞–Ω–∏–º–∏ —Å—Ç–∞—Ç—Ç—ñ
    """
    # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π URL
    if lang == 'ru':
        full_url = f"{OLD_SITE_BASE_URL}/ru{url_path}"
    else:
        full_url = f"{OLD_SITE_BASE_URL}{url_path}"

    print(f"–ü–∞—Ä—Å–∏–Ω–≥: {full_url}")

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.get(full_url, timeout=30, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # –í–∏—Ç—è–≥—É—î–º–æ –¥–∞–Ω—ñ
        title = soup.find('h1')
        title_text = title.get_text(strip=True) if title else ''

        # Meta description
        meta_desc = soup.find('meta', {'name': 'description'})
        meta_description = meta_desc.get('content', '') if meta_desc else ''

        # Canonical URL
        canonical = soup.find('link', {'rel': 'canonical'})
        canonical_url = canonical.get('href', '') if canonical else full_url

        # –ö–æ–Ω—Ç–µ–Ω—Ç
        content_div = soup.find('div', class_='entry-content') or soup.find('div', class_='post-content') or soup.find('article')
        if not content_div:
            # Fallback: —à—É–∫–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
            for div in soup.find_all('div', class_=lambda x: x and 'content' in ' '.join(x).lower() if x else False):
                content_div = div
                break

        if content_div:
            content_html = str(content_div)
        else:
            content_html = ''

        # –î–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó
        from django.utils import timezone as tz
        date_elem = soup.find('div', class_='entry-date') or soup.find('time') or soup.find('div', class_='date')
        published_date = None
        if date_elem:
            date_text = date_elem.get_text(strip=True)
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç–∏ (—Ñ–æ—Ä–º–∞—Ç –º–æ–∂–µ –±—É—Ç–∏ —Ä—ñ–∑–Ω–∏–π)
            try:
                published_date = tz.make_aware(datetime.strptime(date_text, '%d.%m.%Y'))
            except:
                pass

        # Featured image
        featured_img = soup.find('img', class_=lambda x: x and 'wp-post-image' in ' '.join(x) if x else False)
        if not featured_img:
            featured_img = soup.find('div', class_='post-thumbnail')
            if featured_img:
                featured_img = featured_img.find('img')
        if not featured_img:
            featured_img = soup.find('article').find('img') if soup.find('article') else None

        featured_image_url = featured_img.get('src', '') if featured_img else ''

        # Slug –∑ URL
        slug = url_path.split('/news/')[1].rstrip('/') if '/news/' in url_path else ''

        return {
            'title': title_text,
            'slug': slug,
            'content_html': content_html,
            'meta_description': meta_description,
            'canonical_url': canonical_url,
            'published_date': published_date or tz.now(),
            'featured_image_url': featured_image_url,
            'old_url': url_path,
        }
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {full_url}: {e}")
        return None


def check_slug_conflicts(slug_uk, slug_ru=None):
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ slug –Ω–µ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—É—î –∑ —ñ—Å–Ω—É—é—á–∏–º–∏ city/program slugs.

    Returns:
        Tuple (—î_–∫–æ–Ω—Ñ–ª—ñ–∫—Ç, –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è)
    """
    conflicts = []

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑ –º—ñ—Å—Ç–∞–º–∏
    if slug_uk in CITIES:
        conflicts.append(f"–ö–æ–Ω—Ñ–ª—ñ–∫—Ç –∑ –º—ñ—Å—Ç–æ–º: {slug_uk}")

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑ –ø—Ä–æ–≥—Ä–∞–º–∞–º–∏
    if slug_uk in PROGRAMS:
        conflicts.append(f"–ö–æ–Ω—Ñ–ª—ñ–∫—Ç –∑ –ø—Ä–æ–≥—Ä–∞–º–æ—é: {slug_uk}")

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑ –ª–æ–∫–∞—Ü—ñ—è–º–∏
    if slug_uk in LOCATIONS:
        conflicts.append(f"–ö–æ–Ω—Ñ–ª—ñ–∫—Ç –∑ –ª–æ–∫–∞—Ü—ñ—î—é: {slug_uk}")

    if slug_ru:
        if slug_ru in CITIES:
            conflicts.append(f"–ö–æ–Ω—Ñ–ª—ñ–∫—Ç RU slug –∑ –º—ñ—Å—Ç–æ–º: {slug_ru}")
        if slug_ru in PROGRAMS:
            conflicts.append(f"–ö–æ–Ω—Ñ–ª—ñ–∫—Ç RU slug –∑ –ø—Ä–æ–≥—Ä–∞–º–æ—é: {slug_ru}")
        if slug_ru in LOCATIONS:
            conflicts.append(f"–ö–æ–Ω—Ñ–ª—ñ–∫—Ç RU slug –∑ –ª–æ–∫–∞—Ü—ñ—î—é: {slug_ru}")

    return len(conflicts) > 0, conflicts


def import_article(url_path):
    """
    –Ü–º–ø–æ—Ä—Ç—É—î –æ–¥–Ω—É —Å—Ç–∞—Ç—Ç—é (UK —Ç–∞ RU –≤–µ—Ä—Å—ñ—ó).

    Args:
        url_path: –®–ª—è—Ö –¥–æ —Å—Ç–∞—Ç—Ç—ñ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ /news/slug/)
    """
    print(f"\n{'='*60}")
    print(f"–Ü–º–ø–æ—Ä—Ç —Å—Ç–∞—Ç—Ç—ñ: {url_path}")
    print(f"{'='*60}")

    # –ü–∞—Ä—Å–∏–Ω–≥ UK –≤–µ—Ä—Å—ñ—ó
    uk_data = parse_article(url_path, lang='uk')
    if not uk_data:
        print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ UK –≤–µ—Ä—Å—ñ—é")
        return False

    # –ü–∞—Ä—Å–∏–Ω–≥ RU –≤–µ—Ä—Å—ñ—ó
    ru_data = parse_article(url_path, lang='ru')

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—ñ–≤ slug
    has_conflict, conflicts = check_slug_conflicts(uk_data['slug'], ru_data['slug'] if ru_data else None)
    if has_conflict:
        print(f"‚ö†Ô∏è  –£–í–ê–ì–ê: –ö–æ–Ω—Ñ–ª—ñ–∫—Ç–∏ slug: {', '.join(conflicts)}")
        response = input("–ü—Ä–æ–¥–æ–≤–∂–∏—Ç–∏? (y/n): ")
        if response.lower() != 'y':
            return False

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —Å—Ç–∞—Ç—Ç—è –≤–∂–µ —ñ—Å–Ω—É—î
    existing = NewsArticle.objects.filter(slug_uk=uk_data['slug']).first()
    if existing:
        print(f"‚ö†Ô∏è  –°—Ç–∞—Ç—Ç—è –∑ slug_uk='{uk_data['slug']}' –≤–∂–µ —ñ—Å–Ω—É—î. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ.")
        return False

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ–±—Ä–æ–±–∫–∞ –∑–æ–±—Ä–∞–∂–µ–Ω—å
    print("üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å...")
    image_mapping = {}

    # Featured image
    if uk_data.get('featured_image_url'):
        local_path, success = download_and_save_image(
            uk_data['featured_image_url'],
            IMAGE_UPLOAD_PATH
        )
        if success:
            image_mapping[uk_data['featured_image_url']] = local_path
            uk_data['featured_image'] = local_path

    # –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É
    all_images = extract_images_from_html(uk_data['content_html'], OLD_SITE_BASE_URL)
    for img_info in all_images:
        original_src = img_info['original_src']
        webp_src = img_info['src']

        # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        if original_src not in image_mapping:
            local_path, success = download_and_save_image(
                original_src,
                IMAGE_UPLOAD_PATH
            )
            if success:
                # –î–æ–¥–∞—Ç–∏ –≤—Å—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ URL –≤ –º–∞–ø–ø—ñ–Ω–≥
                image_mapping[original_src] = local_path
                image_mapping[webp_src] = local_path  # webp –≤–µ—Ä—Å—ñ—è —Ç–µ–∂ –≤–∫–∞–∑—É—î –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª

                # –î–æ–¥–∞—Ç–∏ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –∑ —Ä–æ–∑–º—ñ—Ä–∞–º–∏ (—è–∫—â–æ —î –≤ srcset)
                # –ù–∞–ø—Ä–∏–∫–ª–∞–¥: image-300x200.jpg -> local_path
                import re
                base_url = original_src.split('?')[0]
                for variant in [original_src, webp_src]:
                    # –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –∑ —Ä–æ–∑–º—ñ—Ä–∞–º–∏
                    size_pattern = r'(\d+)x(\d+)'
                    if re.search(size_pattern, variant):
                        # –î–æ–¥–∞—Ç–∏ –±–∞–∑–æ–≤–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç –±–µ–∑ —Ä–æ–∑–º—ñ—Ä—ñ–≤
                        base_variant = re.sub(r'-\d+x\d+', '', variant)
                        image_mapping[base_variant] = local_path

    # –û—á–∏—â–µ–Ω–Ω—è —Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è HTML
    print("üßπ –û—á–∏—â–µ–Ω–Ω—è HTML...")
    uk_data['content_html'] = clean_wordpress_html(uk_data['content_html'])
    uk_data['content_html'] = update_html_image_urls(uk_data['content_html'], image_mapping)

    if ru_data:
        ru_data['content_html'] = clean_wordpress_html(ru_data['content_html'])
        ru_data['content_html'] = update_html_image_urls(ru_data['content_html'], image_mapping)

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–±'—î–∫—Ç–∞ NewsArticle
    article = NewsArticle(
        slug_uk=uk_data['slug'],
        slug_ru=ru_data['slug'] if ru_data else None,
        title_uk=uk_data['title'],
        title_ru=ru_data['title'] if ru_data else '',
        content_uk=uk_data['content_html'],
        content_ru=ru_data['content_html'] if ru_data else '',
        meta_description_uk=uk_data['meta_description'],
        meta_description_ru=ru_data['meta_description'] if ru_data else '',
        published_at=uk_data['published_date'],
        old_url_uk=uk_data['old_url'],
        old_url_ru=ru_data['old_url'] if ru_data else '',
        is_published=True,
    )

    if uk_data.get('featured_image'):
        article.featured_image = uk_data['featured_image']

    article.save()

    print(f"‚úÖ –°—Ç–∞—Ç—Ç—é —É—Å–ø—ñ—à–Ω–æ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ: {article.title_uk}")
    print(f"   UK URL: {article.get_absolute_url()}")
    if article.slug_ru:
        print(f"   RU URL: /ru{article.get_absolute_url()}")

    return True


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è —ñ–º–ø–æ—Ä—Ç—É."""
    print("üöÄ –ü–æ—á–∞—Ç–æ–∫ —ñ–º–ø–æ—Ä—Ç—É news —Å—Ç–∞—Ç–µ–π –∑—ñ —Å—Ç–∞—Ä–æ–≥–æ —Å–∞–π—Ç—É")
    print(f"   –°—Ç–∞—Ä–∏–π —Å–∞–π—Ç: {OLD_SITE_BASE_URL}")
    print(f"   –ù–æ–≤–∏–π —Å–∞–π—Ç: {NEW_SITE_BASE_URL}")
    print()

    # –û—Ç—Ä–∏–º–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ URL
    news_urls = get_all_news_urls()

    if not news_urls:
        print("‚ùå –°–ø–∏—Å–æ–∫ URL –ø–æ—Ä–æ–∂–Ω—ñ–π. –î–æ–¥–∞–π—Ç–µ URL —Å—Ç–∞—Ç–µ–π –≤ —Ñ—É–Ω–∫—Ü—ñ—é get_all_news_urls()")
        return

    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —è–∫—ñ —Å—Ç–∞—Ç—Ç—ñ –≤–∂–µ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω—ñ
    from apps.core.models import NewsArticle
    existing_slugs = set(NewsArticle.objects.values_list('slug_uk', flat=True))

    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ URL - –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –≤–∂–µ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω—ñ
    urls_to_import = []
    for url_path in news_urls:
        slug = url_path.split('/news/')[1].rstrip('/') if '/news/' in url_path else ''
        if slug and slug not in existing_slugs:
            urls_to_import.append(url_path)

    print(f"üìã –í—Å—å–æ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(news_urls)}")
    print(f"   –í–∂–µ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ: {len(news_urls) - len(urls_to_import)}")
    print(f"   –ü–æ—Ç—Ä—ñ–±–Ω–æ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏: {len(urls_to_import)}")
    print()

    if not urls_to_import:
        print("‚úÖ –í—Å—ñ —Å—Ç–∞—Ç—Ç—ñ –≤–∂–µ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω—ñ!")
        return

    imported = 0
    failed = 0
    skipped = 0

    # –Ü–º–ø–æ—Ä—Ç—É—î–º–æ –ø–æ –æ–¥–Ω—ñ–π —Å—Ç–∞—Ç—Ç—ñ –∑ –∑–∞—Ç—Ä–∏–º–∫–æ—é
    import time
    for i, url_path in enumerate(urls_to_import, 1):
        print(f"\n[{i}/{len(urls_to_import)}] ", end='')
        try:
            result = import_article(url_path)
            if result:
                imported += 1
            else:
                skipped += 1
        except Exception as e:
            print(f"‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê —ñ–º–ø–æ—Ä—Ç—É {url_path}: {e}")
            import traceback
            traceback.print_exc()
            failed += 1

        # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ —ñ–º–ø–æ—Ä—Ç–∞–º–∏ (—â–æ–± –Ω–µ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å–µ—Ä–≤–µ—Ä)
        if i < len(urls_to_import):
            time.sleep(2)  # 2 —Å–µ–∫—É–Ω–¥–∏ –º—ñ–∂ —Å—Ç–∞—Ç—Ç—è–º–∏

        # –ü—Ä–æ–º—ñ–∂–Ω–∏–π –∑–≤—ñ—Ç –∫–æ–∂–Ω—ñ 10 —Å—Ç–∞—Ç–µ–π
        if i % 10 == 0:
            print(f"\nüìä –ü—Ä–æ–º—ñ–∂–Ω–∏–π –∑–≤—ñ—Ç: —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ {imported}, –ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped}, –ø–æ–º–∏–ª–æ–∫ {failed}")

    print()
    print(f"{'='*60}")
    print(f"‚úÖ –Ü–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –£—Å–ø—ñ—à–Ω–æ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ: {imported}")
    print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}")
    print(f"   –ü–æ–º–∏–ª–æ–∫: {failed}")
    print(f"   –í—Å—å–æ–≥–æ –≤ –±–∞–∑—ñ: {NewsArticle.objects.count()}")
    print(f"{'='*60}")


if __name__ == '__main__':
    main()

