#!/usr/bin/env python
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö URL —Å—Ç–∞—Ç–µ–π –∑—ñ —Å—Ç–∞—Ä–æ–≥–æ —Å–∞–π—Ç—É.
"""
import requests
from bs4 import BeautifulSoup
import time

BASE_URL = 'https://speak-up.com.ua/news'
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def get_all_news_urls():
    """–û—Ç—Ä–∏–º—É—î –≤—Å—ñ URL —Å—Ç–∞—Ç–µ–π –∑ —É—Å—ñ—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫."""
    all_urls = []
    page = 1

    while True:
        if page == 1:
            url = f"{BASE_URL}/"
        else:
            url = f"{BASE_URL}/page/{page}/"

        print(f"–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page}...")

        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–∞—Ç—Ç—ñ
            page_urls = []
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Å—Ç–∞—Ç—Ç—ñ (–Ω–µ –∞—Ä—Ö—ñ–≤, –Ω–µ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—é)
                if '/news/' in href and href not in ['/news/', '/news'] and '/page/' not in href:
                    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ URL
                    if href.startswith('/'):
                        full_url = f"https://speak-up.com.ua{href}"
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue

                    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —Ü–µ —Å—Ç–∞—Ç—Ç—è (–º–∞—î slug –ø—ñ—Å–ª—è /news/)
                    path = full_url.replace('https://speak-up.com.ua', '').replace('http://speak-up.com.ua', '')
                    if path.startswith('/news/') and path != '/news/' and len(path.split('/')) >= 3:
                        if full_url not in page_urls:
                            page_urls.append(full_url)

            if not page_urls:
                print(f"–ù–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π. –ó—É–ø–∏–Ω—è—î–º–æ—Å—è.")
                break

            print(f"  –ó–Ω–∞–π–¥–µ–Ω–æ {len(page_urls)} —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page}")
            all_urls.extend(page_urls)

            # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —î –Ω–∞—Å—Ç—É–ø–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞
            next_page = soup.find('a', class_=lambda x: x and 'next' in ' '.join(x).lower() if x else False)
            if not next_page:
                # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–Ω–∞–π—Ç–∏ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—é
                pagination = soup.find_all('a', href=lambda x: x and f'/page/{page + 1}/' in x)
                if not pagination:
                    print(f"–ù–∞—Å—Ç—É–ø–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–µ–º–∞—î. –í—Å—å–æ–≥–æ –∑–Ω–∞–π–¥–µ–Ω–æ {len(all_urls)} —Å—Ç–∞—Ç–µ–π.")
                    break

            page += 1
            time.sleep(1)  # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏

        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page}: {e}")
            break

    # –í–∏–¥–∞–ª–∏—Ç–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∏ —Ç–∞ –≤—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏
    unique_urls = list(set(all_urls))
    unique_urls.sort()

    # –í–∏—Ç—è–≥—Ç–∏ —Ç—ñ–ª—å–∫–∏ —à–ª—è—Ö–∏ (–±–µ–∑ –¥–æ–º–µ–Ω—É)
    paths = []
    for url in unique_urls:
        path = url.replace('https://speak-up.com.ua', '').replace('http://speak-up.com.ua', '')
        if path not in paths:
            paths.append(path)

    return paths

if __name__ == '__main__':
    print("üöÄ –û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö URL —Å—Ç–∞—Ç–µ–π –∑—ñ —Å—Ç–∞—Ä–æ–≥–æ —Å–∞–π—Ç—É...")
    urls = get_all_news_urls()
    print(f"\n‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(urls)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π")
    print("\n–ü–µ—Ä—à—ñ 10 URL:")
    for url in urls[:10]:
        print(f"  {url}")
    print(f"\n... —Ç–∞ —â–µ {len(urls) - 10} —Å—Ç–∞—Ç–µ–π")

    # –ó–±–µ—Ä–µ–≥—Ç–∏ –≤ —Ñ–∞–π–ª
    with open('scripts/all_news_urls.txt', 'w') as f:
        for url in urls:
            f.write(f"{url}\n")
    print(f"\nüíæ URL –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ scripts/all_news_urls.txt")

